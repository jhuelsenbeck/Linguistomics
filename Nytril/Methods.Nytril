using Format, Units, Math, IO, Style, Defs;
//======================================================================

with Figures.ExampleTree {
  Number = 1;

  LanguageList = [
    Languages.Latin,
    Languages.Romanian,
    Languages.Catalan,
    Languages.Portuguese,
    Languages.Spanish,
    Languages.French,
    Languages.Walloon,
    Languages.Friulian,
    Languages.Italian,
  ];

  Tip(i, length)       = new TreeNodeClass("{0} ({1})"(LanguageList[i-1], i), length);
  Branch(name, length) = new TreeNodeClass(name, length);

  Nodes = Branch(17, 1) {
    Tip(1, 7);
    Branch(16, 1) {
      Branch(14, 1) {
        Branch(13, 1) {
          Tip(2, 4);
          Branch(12, 1) {
            Tip(3, 3);
            Branch(11, 1) {
              Branch(10, 1) {
                Tip(6, 1);
                Tip(7, 1);
              };
              Tip(8, 2);
            };
          }
        };
        Tip(4, 5);
      };
      Branch(15, 5) {
        Tip(5, 1);
        Tip(9, 1);
      };
    };
  };

  Body = TreeOptions {
    Root: Nodes;
  };

  Caption = "An example tree showing the relationships of {0} languages."(SampledLanguages(LanguageList.Length));
}
//======================================================================

with Figures.Alignment {
  Number      = 2;
  ArrowWidth  = 50;
  ArrowHeight = 100;
  ArrowMargin = 20 pts;
  Arrowhead   = 8 pts;
  ArrowColor  = 80%;

  HMap1 = [
    "00111110",
    "10001111",
    "10001110",
    "01101111",
    "10101010",
    "10101110",
    "10001111",
    "00001110",
    "10001110",
    "10011111",
  ];

  HMap2 = [
    "111110000",
    "000111100",
    "000111111",
    "000111111",
    "101010100",
    "000101111",
    "000101111",
    "000111000",
    "111100000",
    "000111111",
  ];

  TName(taxon) = Cell {
    Style.SansSerif;
    taxon.Language;
  };

  SegmentTable(aligned) = MatrixBlock {
    TextHeight: 9 pts;

    foreach (var taxon in Results.TaxaArray) {
      Row {
        TName(taxon);
        Cell {
          IPAFamily;
          Span {
            var word = ExampleConcept.Cognate.Words[taxon.Index];
            var segs = aligned ? word.Aligned : word.Segments;
            (each segs).GetBlock;
          }
        };
      }
    }
  };

  AlignSegments(segments, map) = Span {
    IPAFamily;

    var i = 0;
    foreach (var c in map) {
      if (c == '1' and i < segments.Length)
        segments[i++].GetBlock;
      else
        IPA.Segments.GapSegment.GetBlock;
    }
  };

  MapTable(map, text) = MatrixBlock {
    foreach (var taxon in Results.TaxaArray) {
      Row {
        TName(taxon);
        Cell {
          var t = map[EachIndex];
          Span {
            if (text)
              AlignSegments(ExampleConcept.Cognate.Words[taxon.Index].Segments, t);
            else {
              Style.MonoFamily;
              t;
            }
          }
        };
      }
    }
  };

  LabelBox(label) = HBox {
    TextHeight: 18 pts;
    Bold;
    Equation {label};
  };

  ContentBox(content) = HBox {
    MarginL: 4 pts;
    Border: 1 {Color: ArrowColor};
    Padding: 2 pts;
    content;
  };

  ArrowFigure(w, h, reverse) = Canvas {
    Size: Size(w, h);
    Canvas {
      Size: Size(w, h);
      if (reverse)
        Transform: Affine(-1, 0, 0, 1, w, 0);
      Figure {
        Stroke: 1.5 {Color: ArrowColor};
        FigurePath {
          Point(0, 0);
          QuadTo(Point(w*0.92, h*0.4), Point(w - Arrowhead*0.5, h - Arrowhead));
        };
      };
      Figure {
        Fill: ArrowColor;
        FigurePath {
          Closed: true;
          Point(w - Arrowhead, h - Arrowhead);
          LineTo(w, h - Arrowhead);
          LineTo(w - Arrowhead * 0.5, h);
        };
      };
      Canvas {
        Background: Colors.White;
        Position: Point(w * 0.3, h * 0.25);
        TextHeight: 20 pts;
        TextColor: ArrowColor;
        Bold;
        Plus;
      };
    };
  };

  CFigure(c) = Figure {
    Fill: ArrowColor;
    Padding: 1;
    c;
  };

  NumBox(num, c1, c2) = MatrixBlock {
    VAlign: VAligns.Top;
    HAlign: HAligns.Center;
    ColumnGap: 0;
    Row {
      LabelBox(HomologyMap sub num);
      ContentBox(c1);
    };
    Row {
      Empty;
      VBox {
        MarginTB: 6 pts;
        TextHeight: 20 pts;
        HAlign: HAligns.Center;
        CFigure('=');
        CFigure(Tex.downarrow);
      }
    };
    Row {
      LabelBox(SegmentalAlignment sub num);
      ContentBox(c2);
    };
  };

  HomBox(i, map) = NumBox(i, MapTable(map, false), MapTable(map, true));

  Body = VBox {
    HAlign: HAligns.Center;
    HBox {
      MarginB: 4 pts;
      VAlign: VAligns.Bottom;
      ArrowFigure(ArrowWidth, ArrowHeight, true);
      HBox {
        VAlign: VAligns.Top;
        LabelBox(SegmentalInfo);
        ContentBox(SegmentTable(false)) {MarginB: ArrowMargin; MarginR: ArrowMargin};
      };
      ArrowFigure(ArrowWidth, ArrowHeight, false);
    };
    HBox {
      HomBox(1, HMap1);
      Canvas {Width: 1 inches};
      HomBox(2, HMap2);
    }
  };

  Caption = Span {
    "Alignments ";
    InParens {SegmentalAlignment};
    " are formed from the observed segments ";
    InParens {SegmentalInfo};
    " and a homology map ";
    InParens {HomologyMap}; eos;
  };
}
//======================================================================

with StateSection {
  CellPad = Margin(1 pts, 1 pts, 3 pts, 0);
  States  = 'A'..'E' step 1;

  MatrixRow(state) = {
    foreach (var s in States) {
      Cell {
        sym.q sub (state s);
      }
    }
  };

  StateRow(state) = Row {
    Cell {
      HAlign: HAligns.Right;
      Span {
        EachIndex == States.Length div 2 ? Lang.From + Space*3 : Empty;
        state;
      }
    };
    MatrixRow(state);
  };

  StateTable = Table {
    ParAlignment: ParAlignments.Center;

    VAlign: VAligns.Center;
    HAlign: HAligns.Center;
    Padding: CellPad;
    Columns: [(1 inch) {EdgeR: Style.TableEdgeSize}] + 5*[0.5 inches];
    Row {
      Empty;
      Cell {
        ColSpan: States.Length;
        Lang.To;
      };
    };
    Row {
      EdgeB: Style.TableEdgeSize;
      Empty;
      States;
    };
    StateRow(each States);
  };

  StateMatrix = MatrixBlock {
    BracketLR: Brackets.FlatRound;
    foreach (var state in States) {
      Row {
        MatrixRow(state);
      }
    }
  };
}
//======================================================================

with Abstract {
  Title = Lang.Abstract;
  Body  = Block {
    Style.BodyPar {
      "Linguistically phylogenies are standardly inferred on the basis of cognate relationships, which are discrete representations of ancestry. Although inference on the basis of such datasets has yielded important results, it suffers from an obvious fault: it ignores the phylogenetic signal in the segmental form of words. In this paper, we infer the phylogeny of Romance on the basis of segmental data...";
    };
  };
}
//======================================================================

with Content.Introduction {
  Title = Lang.Introduction;
  Body  = Block {
    Style.BodyPar {
      "In this paper, we attempt to do the impossible!";
    };
  };
}
//======================================================================

TreeFigure      = FigureName(Figures.ExampleTree);
AlignmentFigure = FigureName(Figures.Alignment);

with Content.Methods {
  Title = Lang.Methods;
  Body  = Block {
    BodyPar {
      "We use ancestral classes, in which the descendant forms are segmentally aligned. ";
      "It is important to note that our definition of cognate refers only to segmental descent. ";
      "It takes no account of semantics whatsoever. ";
      "So the lexical items for ‘ear’ in Romance are paired with the Latin word "; Word("auricula"); " which is the diminute form of ‘ear’. ";
    };

    BodyPar {
      "Other things we have to mention: "
    };

    NumberList {
      "The data are surface forms, not underlying forms. ";
      "We used the accusative singular for nouns in Latin. ";
    };

    BodyPar {
      "Here, we describe the details of the modeling assumptions we make for the linguistic characters, how we estimate the parameters of the linguists model, and how different models describing the evolution of words can be compared in a statistical framework.";
    };

    BodyPar {
      BodyTitle {"Phylogeny relating languages"}; eos;
      "We assume that modern-day languages are related by an unknown evolutionary tree, called a ";
      SingleQuote {Phylogeny.Name};
      " and denoted "; Phylogeny; eos;
      "The phylogeny contains information on the topological relationships among "; NumLanguages;
      " sampled languages as well as information on the divergence times of the languages or on the amount of change that occurred between the languages. ";
    };

    BodyPar {
      TreeFigure; " shows an example of a phylogeny for "; SampledLanguages(Results.TaxaArray.Length); " languages. ";
      "In the terminology of evolutionary biology, the tree is composed of ‘nodes’ and ‘branches.’ ";
      "(By contrast, mathematicians call nodes and branches ‘vertices’ and ‘edges,’ respectively.) ";
      "The nodes represent the tips of the tree, each of which is assigned a language, and the points on the tree where the languages diverge from one another. ";
      "Each language-assigned tip node is labeled "; (TipLabelNumbers); ". ";
      "The interior nodes are labeled "; TipLabelInterior; " in preorder sequence (i.e., ordered sequentially from the tips to the root). ";
      "The root node is always assigned the label "; TipLastLabel; eos;
      "We denote the ancestor of node "; NodeIndex; " as "; AncestorNode(NodeIndex); eos;
      "In the tree of "; TreeFigure; ", the ancestor of node 3 is "; Equation {AncestorNode(3); Equals; 12}; eos;
    };

    BodyPar {
      "The branches connect the nodes of the tree and are represented as lines in "; TreeFigure; ". ";
      "The branch is assigned the label from its descendant node, so for example in the tree of ";
      TreeFigure;
      ", the branch connecting nodes 12 and 13 is assigned the label 12. ";
    };

    DocumentFigure(Figures.ExampleTree);

    BodyPar {
      "A phylogeny is an information-rich graph. ";
      "For one, it contains information on the relationships of the languages. ";
      "This topological information is denoted "; Topology; eos;
      TreeFigure;
      " for example, suggests that French and Walloon are each others’ closest relatives. ";
      "They are more closely related to each other than they are to any another language on the tree because they share a more recent common ancestor with each other than they do with any other language. ";
      "This common ancestor is the node numbered 10 in "; TreeFigure; eos;
      "French and Walloon, together, are more closely related to Friulian in the tree of "; TreeFigure; eos;
      "Both French and Walloon are equally related to Friulian because they both share the same common ancestor with Friulian, at the node numbered 11 in "; TreeFigure; eos;
      "It is important to realize that there are many possible ways in which the languages can be related to one another, with the tree of ";
      TreeFigure;
      " depicting only one of the possibilities. ";
      "In fact, for the case in which ";
      SampledLanguages(Results.TaxaArray.Length);
      " languages are considered, there are "
      BranchFormula(Results.TaxaArray.Length);
      " possible trees relating the languages. ";
      "In general, the number of possible rooted trees is the product of the odd numbers up to, and including, ";
      MaxBranch;
      ": ";
      BNEquation; eos;
      "Each topology is given a unique label, ";
      Equation {
        EllipsisList {
          Topology sub 1;
          Topology sub 2;
          Topology sub BN;
        }
      }; eos;
      "The number of possible tree topologies becomes quite large very quickly — it is a factorial, after all. ";
      "A linguist interested in the relationships among ";
      SampledLanguages(60);
      " languages, for example, would contend with ";
      Span {BranchFormula(60); TextScientific: true; TextDigits: 2};
      " possible topologies, each depicting a unique and different way the languages can be related. ";
      "For comparison, the number of atoms in the known universe is on the order of "; 10 sup 80; eos;
    };

    BodyPar {
     "Ideally, the linguist would not only be able to estimate the correct topology relating the languages of interest, but also the times at which the languages diverged. ";
     "The interior nodes of the tree represent language divergence events that occurred at specific times in the past and are denoted ";
     Equation {
       Span {Bold; Time};
       Equals;
       InParens {
         EllipsisList {
           Time sub (NumLanguages Plus 1);
           Time sub (NumLanguages Plus 2);
           Time sub TipLastLabel;
         };
       }
     }; eos;

     "The tip nodes are all assigned the time ";
     Equation {
       Time; Equals; 0;
     }; eos;

     "Below, we will discuss in more detail a stochastic model of language change. ";
     "However, the model we use, along with every other stochastic model for phylogenies, has an all-important parameter that describes the rate at which the language changes. ";
     "This parameter is called the substitution rate and is denoted "; SubstitutionRate; eos;

     "Without external information that constrains the divergence times, such as one language divergence time that is considered known, perhaps from textual information, it is impossible to estimate the divergence times. ";
     "The problem is that one obtains the same net divergence between two languages from a high rate of language evolution and a short divergence time separating the languages, or a low rate of language evolution and a long time separating the languages. ";
     "In fact, the expected number of evolutionary events that occurred between two languages that diverged at time "; Time; " is ";
     Equation {
       ExpectedEvents;
       Equals;
       2;
       Time;
       SubstitutionRate;
     }; eos;

     "(The factor of two is introduced because the path between the two languages is the time from one language to the common ancestor, and then back up the tree to the other language.) ";
     "In this paper, we allow each of the "; MaxBranches; " branches of the tree to have an independent substitution rate. ";
     "Hence, the expected number of evolutionary events that occur along the "; Ith; " branch of the tree is ";
     ExpectedEventsPerBranch;
     eos;
     "In this study, we do not estimate the divergence times on the tree, but rather estimate the compound parameter representing the branch lengths (the ";
     Equation {ExpectedEvents sub sym.i};
     " which are in units of expected number of substitutions per segment (see below). ";
    };

    BodyPar {
      "To summarize, we assume languages only diverge from one another, ignoring events such as word assimilation. ";
      "We represent the divergence as a phylogeny containing information on both the topology and branch lengths, together denoted ";
      Equation {
         Phylogeny;
         Equals;
         Arguments {
           Topology;
           ExpectedEvents;
         }
      };
      eos;
      "One of the goals of this study is to estimate these parameters from the data collected from each language."
    };

    BodyPar {
      BodyTitle {"Data"}; eos;
      "The similarities of words from different languages are informative about how the languages are related. ";
      "In this study, we use statistical methods developed in the field of evolutionary biology to estimate the relationships of species based on either the morphological characteristics of the species or the DNA sequences sampled from the same gene and compared across the species. ";
      "The methods assume that the characteristics compared across species are homologous. ";
      "Homology, in evolutionary biology, is similarity in some characteristic that is caused by common ancestry. ";
      "Consider as an example the following DNA sequences sampled from three species,";
    };

    BodyTable {
      Columns: [1 inch, 5 inches];
      AnimalRow(each ExampleAnimals);
    };

    BodyPar {
      "These are partial mitochondrial sequences from "; Style.Citation(References.Gojobori88); eos;
      "In the original paper, the complete data had ";
      SampledLanguages(12);
      " primate species and the sequences were each ";
      // Need to explain what S is
      Equation {"S" Equals 898};
      " nucleotides in length. ";
      "In a phylogenetic analysis of DNA sequenes, homology is assumed at two levels. ";
      "First, one assumes that the sequences that are compared are homologous. ";
      "Typically, homology at this level is established by sequence similarity and synteny of the gene (e.g., the gene that is compared across species is in the same, or at least similar, position along the chromosome when compared across species, which is another way of saying the gene that is compared has the same neighboring genes in all of the species in the analysis). "
    };

    BodyPar {
      "Not only must the DNA sequences be homologous, but the fine-scale homology of the sequences must also be established. ";
      "The DNA sequences, above, are in an aligned form in which the fine-scale homology has been established; ";
      "each column of the alignment is considered to be homologous. ";
      "So, for example, the first column of the alignment which happens to be the nucleotide A in all three species is assumed to be homologous; ";
      "it is assumed that the common ancestor of gorillas, chimpanzees, and humans had the same gene that also had a position that was homologous to the first column in the alignment. ";
      "Fine-scale homology is established using computer programs in a process called ‘alignment.’ ";
      "Importantly, phylogenetic methods assume that the homology established by the alignment program is correct.";
    };

    BodyPar {
      "Linguistic information, of course, is not like biological information. ";
      "In the past, linguists attempted to find homologous words, called cognates. ";
      "Typically, words are chosen that are thought to be resistant to assimilation. ";
      "Variation in the cognate words is carefully scrutinized by the linguist and encoded in a way that computer software, developed with biological character data in mind, can read and produce sensible results. ";
      "The encoding process produces variants on a cognate word with the variant states coded as 0 or 1 (or sometimes more, if there are more than two states for the word). ";
    };

    BodyPar {
      "In this study, we take a different approach. ";
      "Like others, we concentrate on the so-called ‘basic vocabulary’ of a language "
      NeedsReview("SWADESH REFERENCE");
      ", since the lexical items that instantiate concepts in this domain are less prone to horizontal transmission (i.e., linguistic borrowing). ";
      "In contrast to every study of linguistic phylogenetics that we are aware of, however, our investigation draws inferences from segmental information. ";
      "For each concept in our dataset, homologous lexical items are assigned to the same class, which we refer to as a ";
      Definition("cognate class"); eos;
      "The word forms within each cognate class are phonemic representations based on the International Phonetic Alphabet (IPA) ";
      NeedsReview("IPA REFERENCE"); eos;
      "Consider the following word forms for one of the cognate classes for the concept ";
      SingleQuote {ExampleConcept.Name};
    };

    BodyPar {
      Frame {
        SegmentTable(true);
      }
    };

    BodyPar {
      "The word for "; SingleQuote {ExampleConcept.Name};  " in "; ExampleConcept.Taxon.Language; ", for example, is "
      "/"; ExampleConcept.LangWord; "/. ";
      "The matrix for the word "; ExampleConcept.Name; " also assumes that the fine-scale homology of the segments has been established. ";
      "For example, the above matrix for the concept "; SingleQuote {ExampleConcept.Name};  " assumes that the first segment of the ";
      TextList {ExampleConcept.LangNames};
      " languages (respectively, ";
      TextList {
        Style.IPAFamily;
        ExampleConcept.FirstLetters
      };
      ") are homologous. ";
      "The dashes indicate that there is no homologous segment at that potential position in the word. ";
      "Note that the first segment for the word ";
      ExampleConcept.Name;
      " in "; ExampleConcept.Callout.Language; " is also "; ExampleConcept.CalloutSegment; eos;
      "Why wasn't the first segment from Italian considered to be homologous to the first segments in ";
      TextList {ExampleConcept.LangNames};
      "? ";
      "In this case, the alignment program chose the alignment that did not consider ";
      SingleQuote {ExampleConcept.CalloutSegment};
      " of "; ExampleConcept.Callout.Language; " as homologous to the ";
      SingleQuote {ExampleConcept.CalloutSegment};
      " of French based on the settings chosen by the user. ";
      "It may be that other alignments are nearly as good as the one that is illustrated. ";
      AlignmentFigure;
      " shows two possible alignments of the word "
      ExampleConcept.Name;
      eos;
    };

    BodyPar {
      "Note that a segmental alignment, denoted ";
      SegmentalAlignment;
      ", is constructed by combining the word segment information for the languages of interest with information on the homology of the segments. ";
      "The segmental information for the ";
      NumLanguages;
      " languages of interest is denoted ";
      Equation {
        SegmentalInfo;
        Equals;
        InParens {
          EllipsisList {
            Segment sub 1;
            Segment sub 2;
            Segment sub NumLanguages;
          }
        }
      };
      ", where ";
      Defs.Segment sub sym.i;
      " is the segmental string for the ";
      Ith;
      " language. ";
      "The segmental information for the ";
      SampledLanguages(Results.TaxaArray.Length);
      " languages for the cognate class above for ";
      SingleQuote {ExampleConcept.Name};
      " is:";
    };

    BodyPar {
      Frame {
        SegmentTable(false);
      }
    };

    BodyPar {
      "Here, the segments for "; ExampleConcept.Taxon.Language; " would be ";
      Equation {
        Segment sub 1;
        Equals;
        InParens {
          ExampleConcept.LangWord
        }; eos;
      };
      "The alignment of the segments is accomplished by the use of a map, ";
      HomologyMap;
      ", describing the homology of the word segments. ";
      "The alignment is formed by combining the word segment information with the homology map, ";
      Equation {
        SegmentalAlignment;
        Equals;
        Arguments {
          SegmentalInfo;
          HomologyMap;
        };
      }; eos;
      AlignmentFigure;
      " shows an example of two alignments for the word ";
      ExampleConcept.Name;
      " that can be formed using two different homology maps. ";
    };

    DocumentFigure(Figures.Alignment, 9 pts);

    BodyPar {
      "Ultimately, the linguist observes the phonetic (in the case of contemporary languages) or graphemic (in the case of corpus languages) forms of words, on the basis of which phonemic representations are posited. ";
      "The alignment, by contrast, is not directly observed. ";
      "There are many different ways in which the segments from a cognate word can be homologous. ";
      "The example from the word "; ExampleConcept.Name; ", above, shows only one such way. ";
      "In this study, we develop the statistical and analytical machinery that allow us to marginalize over the segmental alignments. ";
      "Our method considers all possible segmental alignments of the word forms in a cognate class, weighting each such possibility by its probability under a model. ";
      "In this way, our method does not condition on any specific segmental alignment.";
    };

    BodyPar {
      BodyTitle {"Language evolution model"}; eos;
      "We assume that cognate words evolve along the branches of a phylogenetic tree through substitution of one segment by another, insertion of a new segment, or deletion of a segment.";
    };

    BodyPar {
      TextColor: Colors.Red;
      LeftIndent: 0.25 inches;
      "The process of linguistic change is more complex than this sentence allows. ";
      "There are cases where entire words disappear and emerge. ";
      "There are also cases in which the form of word can change with addition or deletion of a block of segments (called a morpheme). ";
      "If we want to restrict our scope to the forms of change mentioned at the beginning of this paragraph (i.e., to segmental transitions), we can do that, but that will impact the data that I collect. ";
      "We would also need to make it explicit that we are by design excluding certain types of well-known linguistic change.";
    };

    BodyPar {
      "Substitution of one segment for another is modeled using a continuous-time Markov model in which the possible states are the set of segments in the phonemic representations. ";
      "At the heart of a continuous-time Markov chain is a rate matrix describing the rate of change between all pairs ofstates. ";
      "As an example, consider a simplified Markov process with only five segments as states. ";
      "The rates of change between the pairs of states can be represented in table form as";
    };

    FigureBody {
      StateSection.StateTable;
    };

    BodyPar {
      "where ";
      Equation {
        sym.qij;
        Tex.geq;
        0;
        InParens {
          sym.i;
          Tex.neq;
          sym.j;
        }
      };
      " is the rate of change from segment "; sym.i; " to segment "; sym.j; eos;
      "The diagonal elements of the rate matrix "; sym.qii; " are specified such that each row sums to zero (i.e., ";
      Equation {
        sym.qii;
        Equals;
        Negative;
        Summation {
          Lower: Span {sym.j Tex.neq; sym.i};
          sym.qij;
        }
      };
      "); this negative value can be interpreted as the rate at which the process moves away from state "; sym.i; eos;
      "Note that the information on rates of change between all pairs of states is not typically represented in table form, but rather in matrix form as";
    };

    EquationPar {
      RateMatrix;
      Equals;
      InBraces {sym.qij};
      Equals;
      StateSection.StateMatrix;
      Space;
      RateMatrixScale;
    };

    BodyPar {
      "Here, we introduce an additional parameter, ";
      RateMatrixScale;
      ", that scales the rate matrix such that the average rate of segmental substitution is one.";
    };

    BodyPar {
      "A continuous-time Markov model has a simple physical interpretation. ";
      "Specifically, when the process is in state "; sym.i; ", one waits an exponentially-distributed time with parameter ";
      Equation {Negative; sym.qii}; " until the next  segmental substitution occurs. ";
      "The change, when it occurs, is to state ";
      Equation {sym.j};
      " with probability ";
      Equation {
        Negative;
        sym.qij; Divide; sym.qii;
      } eos;
      "Several important quantities can be calculated using the information contained in the rate matrix, ";
      RateMatrix; eos;
      "For one, the probability the process ends in state ";
      Equation {sym.j};
      " conditional on starting in state ";
      Equation {sym.i};
      " after a period ";
      Equation {ExpectedEvents};
      " can be calculated through exponentiation of the rate matrix, ";
      Equation {
        StateProbability;
        InParens {ExpectedEvents};
        Equals;
        Nary {
          Operator: sym.e;
          Upper: Span {RateMatrix; ExpectedEvents};
        }
      }; eos;
      "One can also calculate the equilibrium distribution of the process --- denoted ";
      Equation {EquilibriumDistribution};
      " and interpreted as the probability of capturing the process in a particular state after a very long time (formally, an infinite amount of time) has passed --- by solving the system of equations defined by ";
      Equation {
        EquilibriumDistribution;
        RateMatrix;
        Equals;
        Bold 0;
      } eos;
      "Both the transition probabilities and equilibrium probability distribution play an important role in calculating the likelihood (see below).";
    };

    BodyPar {
      "Insertions and deletions of single segments occur at rates ";
      InsertDelete(" and ");
      ", respectively. ";
      "Consider, for a moment, the long-term behavior of a process in which ";
      InsertDelete(" > "); eos;
      "On average, segments would be inserted more frequently than they would be deleted. ";
      "Word forms, then, would evolve to become a mouthful, growing without bounds. ";
      "The opposite situation occurs when ";
      InsertDelete(" < ");
      ", in which segments are deleted at a higher rate than they are inserted. ";
      "In this case, words would be whittled down to nothing; ";
      "a language speaker would not find the words to describe anything, even important people in the person's life, such as the person who gave birth to him or her! ";
    };

    BodyPar {
      ThornEtAl; " described a model of DNA sequence evolution that allowed single nucleotides to be inserted and deleted at rates ";
      InsertDelete(" and "); eos;
      "They introduced a convention for thinking about a DNA sequence in which nucleotides were connected by invisible links. ";
      "Each nucleotide paired with the link to its right. ";
      "The left-most nucleotide had to its left a special link, termed the immortal link. ";
      "Importantly, when a nucleotide was inserted, it was inserted to the right of a link and brought along its own link (to its right). ";
      "Deletions removed a nucleotide and the paired link. Importantly, the immortal link is never deleted. ";
      "So, in the event that "; InsertDelete(" < "); ", the process does not actually go extinct. ";
      "Rather, a nucleotide (and its link) can be inserted to to the right of the immortal link. ";
      "In the model described by "; ThornEtAl; " they constrain the insertion rate to be less than the deletion rate ";
      InsertDelete(" < "); eos;
      "The equilibrium distribution of a sequence length is then geometrically-distributed with parameter ";
      InsertDelete(" / "); eos;
      "We follow the convention of "; ThornEtAl; " in this study. ";
      "In fact, the model we use is precisely the same as the "; ThornEtAl; " model, but with a different continuous-time Markov model used to describe segmental transitions (instead of the four-state process describing nucleotide substitutions used by Throne et al., 1991)."
    };

    BodyPar {
      "The overall substitution and insertion/deletion process can be described as follows. ";
      "In a sequence ";
      WordLength;
      " segments in length, of which ";
      WordLength sub sym.i;
      " of them are of segment type ";
      Equation {sym.i};
      ", the time until the next event occurs is exponentially-distributed with parameter ";
    };

    EquationPar {
      InDelDistribution;
    };

    BodyPar {
      "When an event occurs, it is a substitution with probability ";
      Equation {
        InDelProbability;
      };
      ", an insertion with probability ";
      Equation {
        WordLength;
        InsertionRate;
        Divide;
        RateParameter;
      };
      ", and a deletion with probability ";
      Equation {
        InParens {
          WordLength;
          Minus;
          1;
        };
        DeletionRate;
        Divide;
        RateParameter;
      }; eos;
      "Importantly, the process allows the alignment map ";
      Equation {
        InParens {HomologyMap};
      };
      " to be treated as a parameter of the model. ";
    };

    BodyPar {
      BodyTitle {"Bayesian estimation of language evolution model parameters"}; eos;
      "We estimate the parameters of the language-evolution model in a Bayesian framework. ";
      "Bayesians base inferences on the posterior probability distribution of a parameter, which can be calculated using Bayes' theorem as";
    };

    EquationPar {
      PPO;
      Equals;
      Fraction {
        Span {
          POP;
          Space;
          PP;
        };
        PO;
      }
    };

    BodyPar {
      "where the vertical bar indicates a conditional statement. ";
      "In words, the posterior probability distribution of the parameters is equal to the likelihood ";
      Equation {InBrackets {PPO}};
      " times the prior probability distribution ";
      Equation {InBrackets {PP}};
      ", divided by the marginal likelihood ";
      Equation {InBrackets {PO}}; eos;
    };

    BodyPar {
      "In this study, parameters include:";
    };

    BodyTable {
      Columns: [1 inches {HAlign: HAligns.Right}, 20 pts, 4 inches];
      ParamRow {
        ParamList(Topology);
        "Tree Topologies";
      };
      ParamRow {
        ParamList(ExpectedEvents);
        "Branch length parameters";
      };
      ParamRow {
        Equation {RateMatrixParams};
        Span {RateMatrixParams.Description; Space; Equation {RateMatrix}};
      };
      ParamRow {
        Equation {InsertDelete(", "); Space; InParens {InsertDelete(" < ")}};
        "The insertion and deletion rates of  segments";
      };
      ParamRow {
        Equation {HomologyMap};
        HomologyMap.Description;
      };
    };

    BodyPar {
      "We assign prior probability distributions to all parameters of the model (see Table 1). ";
      "The posterior probability distribution of the linguistic model parameters is then";
    };

    EquationPar {
      Prob(CommaList {
        Topology;
        ExpectedEvents;
        RateMatrixParams;
        InsertionRate;
        Span {DeletionRate; Tex.vert; SegmentalInfo};
      });
      Equals;
      Fraction {
        Span {
          ProbVar(SegmentalInfo);
          Space;
          ProbALL;
        };
        Prob(SegmentalInfo);
      }
    };

    BodyPar {
      "Note that the likelihood is marginalized over all possible alignments, "
    };

    EquationPar {
      ProbVar(SegmentalInfo);
      Equals;
      Summation {
        Lower: HomologyMap;
        ProbVar(SegmentalAlignment);
        Space;
        Prob(CommaList {Span {SegmentalAlignment; Tex.vert; HomologyMap}; SegmentalInfo});
        Space;
        Prob(HomologyMap);
      };
    };

    BodyPar {
      "where the sum is over all possible alignment maps, which implies that our inferences are not conditioned on any particular alignment of segments being correct. ";
      "Similarly, the marginal likelihood accounts for all possible combinations of model parameters:";
    };

    EquationPar {
      Prob(SegmentalInfo);
      Equals;
      Summation {Lower: Topology};
      IntegrateOver(ExpectedEvents);
      IntegrateOver(RateMatrixParams);
      IntegrateOver(InsertDelete(" < "));
      ProbVar(SegmentalInfo);
      Space;
      ProbALL;
      D(ExpectedEvents);
      D(RateMatrixParams);
      D(InsertionRate);
      D(DeletionRate);
    };

    BodyPar {
      "where the integrals represent integration over all possible combinations of branch lengths, rate matrix parameters, and insertion/deletion rates.";
    };

    BodyPar {
      "We calculate the likelihood on a per-word basis using the algorithm described by Lunter et al. (2003) that conditions on an alignment. ";
      "Although the posterior probability distribution can be written down, and individual components such as the prior probability or likelihood for a particular combination of parameters can be calculated, analytically solving the high dimensional summations and integrals required for the posterior probability is unfeasible. ";
      "Instead, we numerically approximate the joint posterior probability distribution of the parameters using Markov chain Monte Carlo.";
    };

    BodyPar {
      BodyTitle {"Markov chain Monte Carlo"}; eos;
      "The aim with Markov chain Monte Carlo (MCMC) is to construct a Markov chain that has as its possible states the parameter values of the statistical model and a stationary distribution that is the posterior probability distribution of the parameters. ";
      "Metropolis et al. (1953) and Hastings (1970) described rules that allow the scientist to construct such a chain. ";
      "When at stationarity, samples from this chain form valid, albeit dependent, samples from the posterior probability distribution. ";
      "The Metropolis-Hastings algorithm constructs the Markov chain using the following algorithm:";
    };

    NumberedList {
      Paragraph {
        "The current state of the chain is denoted ";
        RateMatrixParams; eos;
        "If this is the first cycle of the Markov chain, initialize ";
        RateMatrixParams;
        ", perhaps by choosing a value from the prior distribution.";
      };

      Paragraph {
        "Propose a new value for "; RateMatrixParams; " denoted "; Proposal; eos;
        "The proposal mechanism is up to the programmer, but must involve the generation of random numbers ";
        RandomNumber;
        " such that the proposed value is a function of the current value and the random numbers, ";
        Equation {
          Proposal;
          Equals;
          "h";
          Arguments {
            RateMatrixParams;
            RandomNumber;
          };
        }; eos;

        "The probability of proposing the new value is ";
        NewProposal(RateMatrixParams, RateMatrixParams);
        "whereas the probability of the imagined reverse move, not actually made in computer memory, is ";
        NewProposal(Proposal, RateMatrixParams);
      };

      Paragraph {
        "Calculate the probability of accepting the proposed value:";
        LineBreak;
        Equation {
          Tab;
          ProbOfAccepting;
          Equals;
          "min";
          InParens {
            "1, ";
            Fraction {
              Fof(Span {Unknown; Tex.vert; Proposal});
              Fof(Span {Unknown; Tex.vert; RateMatrixParams});
            };
            Times;
            Fraction {
              Fof(Span {Proposal});
              Fof(Span {RateMatrixParams});
            };
            Times;
            Fraction {
              NewProposal(Proposal, RateMatrixParams);
              NewProposal(RateMatrixParams, Proposal);
            };
          };
        };
        LineBreak;
        "In words, the acceptance probability is the product of the likelihood, prior, and proposal ratios.";
      };

      Paragraph {
        "Generate a uniform(0,1) random variable, ";
        Equation {
          RandomNumber
        }; eos;
        "If ";
        Equation {
          RandomNumber;
          " < ";
          ProbOfAccepting;
        };
        ", accept the proposed state, setting ";
        Equation {
          RateMatrixParams;
          Equals;
          Proposal;
        }; eos;
        "Otherwise, the proposed state is said to be rejected and the chain remains in state ";
        Equation {
          RateMatrixParams;
        };
      };

      Paragraph {
        "Return to Step # 1.";
      };
    };

    BodyPar {
      "The proposals we implement in this study are all typical for phylogenetic models. ";
      NeedsReview("Some details here");
      "The unique aspect of this study is a proposal mechanism for the alignments of the segments for various words. ";
      "Here, we use the proposal mechanism described by Lunter et al. (2005). ";
    };

    BodyPar {
      NeedsReview("A bit on interpretation of MCMC results here.");
    };

    BodyPar {
      BodyTitle {"Model comparison"}; eos;
      "In a Bayesian analysis, parameter estimates are based on the joint posterior probability distribution of the parameters, which we numerically approximate using the Metropolis-Hastings algorithm. ";
      "Often, however, the linguist is interested in the comparison of two or more models with the goal of evaluating which of the models best explains the observations. ";
      "Bayesian model comparison is based on the marginal likelihoods of the models. ";
      "Consider two different linguistics models, ";
      Model sub 1; " and "; Model sub 2;
      " with marginal likelihoods, "; PSM(1); " and "; PSM(2);
      " (note the marginal likelihoods are calculated for the same observations). The ratio of the marginal likelihoods,";
    };

    EquationPar {
      BayesFactor sub 12;
      Equals;
      Fraction {
        PSM(1);
        PSM(2);
      }
    };

    BodyPar {
      "called the Bayes Factor, measures the relative support of the two models; ";
      "a Bayes Factor less than one favors ";
      Model sub 1;
      " whereas the oppose is true for a Bayes factor greater than one. ";
      "Unlike in frequentist statistics, one does not obtain p-values in a Bayesian comparison of models. ";
      "Rather, the Bayes Factor is interpreted as is, or on a log scale. ";
      "Jeffreys (1961) provided a table to help with the interpretation of Bayes Factors:";
    };

    BodyTable {
      Columns: [1 inches, 1 inches, 3 inches];
      Row {
        EdgeB: 0.5 pts;

        BayesFactor;
        Span {"log" sub 10; BayesFactor};
        "Interpretation";
      };
      Row {
        Span {1; Minus; 3.2};
        Span {0; Minus; 1/2};
        "Not worth a bare mention";
      };
      Row {
        Span {3.2; Minus; 10};
        Span {1/2; Minus; 1};
        "Substantial";
      };
      Row {
        Span {10 Minus 100};
        Span {1 Minus 2};
        "Strong";
      };
      Row {
        Span {" > "; 100};
        Span {" > "; 2};
        "Decisive";
      };
    };

    BodyPar {
      "In a Bayesian analysis, there is no need to penalize parameter rich models for having more parameters. ";
      "Rather, the penalization is built into the comparison; ";
      "the additional parameters in a complicated model are each assigned a prior probability distribution. ";
      "A parameter-rich model has lower prior probability for any combination of model parameters than a simpler model. ";
      "Hence, there is no need to compare the Bayes factor to a null distribution as there is in frequentist hypothesis testing.";
    };

    BodyPar {
      "The main limitation of Bayesian model comparison is numerically approximating the marginal likelihoods of the models. ";
      "This can be done in numerous ways. ";
      "For example, one can construct a Markov chain that jumps between models, even if the models differ in dimensions, using a generalization of MCMC described by Green (1995; reversible-jump MCMC). ";
      "Alternatively, one can numerically approximate the marginal likelihoods using what is called path-sampling in which numerous MCMC chains explore a path between the prior and posterior distributions ";
      NeedsReview("citations"); eos;
    };

    BodyPar {
      "In this study, we compare two models. ";
      "The first model is the simplest one we could devise, assuming that the rate of change between all segments is equal. ";
      "Our first model is isomorphic to the earliest model of DNA substitution, called the Jukes and Cantor model in molecular evolution (Jukes and Cantor, 1969). "
      "The second model assumes that the rate of change to ";
      NeedsReview("something");
      " is potentially different than the rate of change to ";
      NeedsReview("something else");
      ":";
    };

    BodyTable {
      Columns: [0.5 inches, 5 inches];
      Row {
        Span {Model sub 1; ":"};
        Equation {
          sym.qij;
          Equals;
          Tex.alpha;
          " (i.e., all elements of rate matrix are equal)";
        };
      };
      Row {
        Span {Model sub 2; ":"};
        Equation {
          sym.qij;
          Equals;
          VBox {
            BracketL: Brackets.FlatCurly;
            IfValue(Tex.alpha, " is a something");
            IfValue(Tex.beta, " is a something else");
          };
        };
      };
    };
  };
}

IfValue(a, text) = Span {
  a;
  " if ";
  sym.i;
  Space;
  Tex.rightarrow;
  Space;
  sym.j;
  text;
};

IntegrateOver(v) = Integral {
  TextStacked: false;
  Lower: v;
};

ParamList(v) = Equation {EllipsisList {v sub 1; v sub BN}};

D(x) = HBox {Extra " d"; x};

Fof(x) = Equation {
  Span {Italic; "f"};
  InParens {
    x;
  }
};

PSM(n) = Equation {
  MarginalLikelihood;
  InParens {
    SegmentalInfo;
    Tex.vert;
    Model sub n;
  }
};

NewProposal(start, end) = Equation {
  ProbProposal;
  InParens {
    start;
    Space;
    Tex.rightarrow;
    Space;
    end;
  }
};

ProbVar(x) = Prob(CommaList {
  Span {x; Tex.vert; Topology};
  ExpectedEvents;
  RateMatrixParams;
  InsertionRate;
  DeletionRate;
});

ProbALL = Prob(CommaList {
  Topology;
  ExpectedEvents;
  RateMatrixParams;
  InsertionRate;
  DeletionRate;
});

ParamRow = Row {
  Separator: Empty;
};

ThornEtAl = "Thorne et al. (1991)";

Prob(x) = HBox {
  Span {Extra; "P"}; InParens {x};
};

PPO = Prob("Parameter(s) | Observations");
POP = Prob("Observations | Parameter(s)");
PP  = Prob("Parameter(s)");
PO  = Prob("Observations");

InsertDelete(by) = Equation {
  InsertionRate;
  by;
  DeletionRate;
};

SegmentTable(aligned) = BodyTable {
  TextHeight: 10 pts;
  Columns: [1.5 inches]*2;

  var words = ExampleConcept.Cognate.Words;
  foreach (var taxon in Results.TaxaArray) {
    Row {
      Cell {
        taxon.Language;
      };

      Cell {
        IPAFamily;
        Span {
          var list = aligned ? words[taxon.Index].Aligned : words[taxon.Index].Segments;
          (each list).GetBlock;
        }
      };
    }
  }
};
//======================================================================

with Content.Conclusion {
  Title = Lang.Conclusion;
  Body  = Block {
    Paragraph {
      "Vene Vidi Vici";
    };
  };
}
//======================================================================

